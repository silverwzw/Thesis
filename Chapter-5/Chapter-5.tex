\chapter{Snapshot Performance}
\label{chap:perform}

    The space occupied by a snapshot is an important measurement of how efficient the snapshot system is. In this chapter, we will focus on the actual space occupied by the snapshot using Kabi file system.
    
    A section that is not referring to a whole block like the third section in \fref{fig:rsync} will be named ``tuncated section'' here after.

    To monitor the space occupied by a snapshot, we will do experiments with the following steps:

\begin{enumerate}
	\item Initialize the file system with block size $B$.

	\item Generate a file with size $F$.

	\item Fill the file with sections and let a certain proportion ($P$) of the sections be truncated.

	\item Take a snaphot of the file.
	
	\item Insert or overwrite random number of bytes to the file at random offset. The offset variable will be uniformly distributed between $0$ and $F$. The number of bytes being overwritten will be uniformly distributed from $1$ to largest possible value (i.e., until the end of the file).

	\item Take another snapshot and calculate the total space occupied by the second snapshot.

	\item Repeat above steps for 10,000 times to collect data (the average value).
\end{enumerate}

    \tref{tab:sample_result} shows two sample results of such experiment. The first three columns in the table are the variables of the experiment. The later four columns are the data gathered from the experiment. For example, the column labeled ``overwrite, classic'' means correspnding write operation is an overwrite and the algrithm used by snapshot system is classic copy-on-write.

    The first row in \tref{tab:sample_result} represents an experiment on a Kabi file system with a 128-byte block size. The target file is a 12,608-byte file with 3\% of all sections truncated. The result shows that on average it takes a classic copy-on-write snapshot system 3,288 bytes to take a snapshot after an overwrite operation. It takes 103 bytes more for a Kabi File System to take a snapshot under the same condition. When it comes to insertion, on average it only takes the Kabi File System 3,256 bytes to take a snapshot after an insertion while it costs almost 2 times more space for a classic copy-on-write snapshot system to do so.

    This result is not difficult to explain. Because there's not much duplicated data in the overwrite scenario, the SHA hashs and rolling checksums become overheads. This makes the performance of Kabi File System a little lower than the classic approach. But in the insertion scenario, lots of duplicated data can be found. The rsync algorithm is able to find the duplications to improve the efficiency. On the contrary, the classic approach can not do anything. So the Kabi File System have a better perfromance  when it comes to insertion.
    
    The second row in \tref{tab:sample_result} represents another experiment different from the first one in file size. The file size in the second row equals to 126,080 bytes. It is obvious that when the file size becomes larger, the number space used to take a snapshot also become larger. However, this makes the comparasion between two experiments of different file size not so obvious.Therefore We introduce the normalized data as shown in \tref{tab:norm}. The major difference between \tref{tab:norm} and \tref{tab:sample_result} is that in \tref{tab:norm} we use the size of the file to normailze the result data so that we can compare results from different experiments easier.

\begin{lscape} 
\begin{table}
\caption{Sample result of the experiment}
\label{tab:sample_result}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12608 & 3\% & 3288 & 3391 & 9530 & 3256 \\
\hline
128 & 126080 & 3\% & 31722 & 32205 & 94867 & 32557 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{Normalized data}
\label{tab:norm}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12608 & 3\% & 0.2607 & 0.2689 & 0.7558 & 0.2582 \\
\hline
128 & 126080 & 3\% & 0.2516 & 0.2554 & 0.7524 & 0.2582 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\section{Block Size and File Size}

    The block size and the file size also influence the efficiency of a snapshot system. A high file-size-to-block-size ratio means fine-grained blocks. A classic copy-on-write snapshot will get better efficiency if it is a fine-grained target file. Consider the extreme case where the file-size-to-block-size ratio is 1. Then the file contains exactly one block. So any change means a complete rewrite to the file by classic copy-on-write strategy. Column 4 and 6 in \tref{tab:fb_ratio} reflects this influence where high file-size-to-block-size retio improves the efficiency of classic copy-on-write system.

    However, column 7 shows that there's no obvious relationship between file-size-to-block-size ratio and efficiency when doing an insertion in Kabi File System. One reason for this could be, the kabi file system can tuncate block into smaller section freely as shown in \fref{fig:buffer}. Therefore the Kabi File System can have fine-grained sections even though the block size is large. 
    
    The data in column 5 reflects the fact that an increase in file-size-to-block-size ratio will result in an improvement in efficiency. The reason could be that the proportion of the metadata reduces when the file size increases. For example, an overwrite operation can result in at most two additional section added to the file node. This means an extra 56-byte metadata. Compared to a 704 byte file, such overhead is huge. But to a 14,008-byte file this overhead can be omitted.

\begin{lscape} 
\begin{table}
\caption{File Size to Block Size ratio}
\label{tab:fb_ratio}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 704 & 17\% & 0.4531 & 0.5127 & 0.8338 & 0.2983 \\
\hline
128 & 1408 & 17\% & 0.3514 & 0.3934 & 0.7933 & 0.2962 \\
\hline
128 & 2112 & 17\% & 0.3129 & 0.3532 & 0.7789 & 0.2964 \\
\hline
128 & 3520 & 17\% & 0.2884 & 0.3213 & 0.7702 & 0.2972 \\
\hline
128 & 7040 & 17\% & 0.2681 & 0.2949 & 0.7589 & 0.2955 \\
\hline
128 & 14008 & 17\% & 0.2593 & 0.2828 & 0.7545 & 0.2957 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\section{Truncate Ratio}

    The rsync algorithm searches duplication in local buffer and remote sections. It uses the rolling checksum of the section to find the duplication. But a truncated section does not have a valid rolling checksum thus it cannot be benifited from the rsync algorithm. We can infer that the more truncated sections we have, the less efficient the snapshot system will be.

    \tref{tab:truncate_ratio} supports this inference. It shows the relationship between the efficiency and tuncated sections. Since a classic copy-on-write snapshot system does not have ``truncated section'', it is clear from the table that they are not infulenced by this experiment variable. On the other hand, the tuncated sections significantly influence the efficiency of Kabi File System. It makes rsync algorithm less powerful. When all sections are truncated, the rsync algorithm no longer provide any extra efficiency in insert operation.

    In this experiment, we explicilty set the size of tuncated section to be half of the block size. That means for a Kabi File System with a 128-byte block size, the overhead (28 bytes) in metadata is almost half of the data contains in truncated section. In other words, for every 2 byte stored in tuncated section, there is 1 extra byte cost in metadata. Since the increase in truncated-to-untruncated ratio implies more truncated section which is less efficient, this explains the significent decrease in efficiency in column 5 of \tref{tab:truncate_ratio}.

\begin{lscape} 
\begin{table}
\caption{Truncate Ratio}
\label{tab:truncate_ratio}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12800 & 0\% & 0.2596 & 0.2645 & 0.7547 & 0.2495 \\
\hline
128 & 12800 & 10\% & 0.2600 & 0.2752 & 0.7558 & 0.2750 \\
\hline
128 & 12800 & 18\% & 0.2599 & 0.2856 & 0.7557 & 0.3002 \\
\hline
128 & 12800 & 33\% & 0.2606 & 0.3071 & 0.7569 & 0.3513 \\
\hline
128 & 12800 & 46\% & 0.2616 & 0.3290 & 0.7579 & 0.4026 \\
\hline
128 & 12800 & 57\% & 0.2616 & 0.3500 & 0.7583 & 0.4532 \\
\hline
128 & 12800 & 71\% & 0.2596 & 0.3792 & 0.7530 & 0.5252 \\
\hline
128 & 12800 & 82\% & 0.2579 & 0.4089 & 0.7505 & 0.5980 \\
\hline
128 & 12800 & 91\% & 0.2593 & 0.4417 & 0.7536 & 0.6758 \\
\hline
128 & 12800 & 100\% & 0.2601 & 0.4737 & 0.7561 & 0.7536 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\section{Conclusion}

    In this Chapter, we showed the effects of three importent factors that affect the snapshot efficiency. The three factors are the block size, the file-size-to-block-size ratio and the proportion of truncated sections.

    Generally, in Kabi snapshot system an increase in block size or an increase in file-size-to-block-size ratio will make the overheads of metadata more efficient. The proportion of truncated sections is the most important factor among the three factors and an increase in this proportion will significently decrease the efficiency of the snapshot system in Kabi File System.
