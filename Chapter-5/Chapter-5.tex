\chapter{Performance}
\label{chap:perform}

    As an important resource management component, the perforamnce of a file system has a large influence on the operating system.In this chapter, we will evaluate the performance of the system. We will evaluate the performance of the file system in terms of time complexity and the performance of snapshot system in terms of space efficiency.

\section{File System Benchmark}

In this section we will evaluate the performance of the Kabi File System using the file system benchmark tool ``postmark''\cite{postmark} 

===========native - continues==============

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	59 seconds total
	5 seconds of transactions (100 per second)

Files:
	5249 created (88 per second)
		Creation alone: 5000 files (96 per second)
		Mixed with transactions: 249 files (49 per second)
	268 read (53 per second)
	232 appended (46 per second)
	5249 deleted (88 per second)
		Deletion alone: 4998 files (2499 per second)
		Mixed with transactions: 251 files (50 per second)

Data:
	1.40 megabytes read (24.30 kilobytes per second)
	26.65 megabytes written (462.53 kilobytes per second)

=========local Kabi - continues===========

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	256 seconds total
	27 seconds of transactions (18 per second)

Files:
	5249 created (20 per second)
		Creation alone: 5000 files (34 per second)
		Mixed with transactions: 249 files (9 per second)
	268 read (9 per second)
	232 appended (8 per second)
	5249 deleted (20 per second)
		Deletion alone: 4998 files (58 per second)
		Mixed with transactions: 251 files (9 per second)

Data:
	1.40 megabytes read (5.60 kilobytes per second)
	26.65 megabytes written (106.60 kilobytes per second)


=======local Kabi - small ===============

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	16 seconds total
	8 seconds of transactions (62 per second)

Files:
	764 created (47 per second)
		Creation alone: 500 files (83 per second)
		Mixed with transactions: 264 files (33 per second)
	243 read (30 per second)
	257 appended (32 per second)
	764 deleted (47 per second)
		Deletion alone: 528 files (264 per second)
		Mixed with transactions: 236 files (29 per second)

Data:
	1.36 megabytes read (87.33 kilobytes per second)
	4.45 megabytes written (284.55 kilobytes per second)

=======WLAN Kabi========================
Time:
	676 seconds total
	347 seconds of transactions (1 per second)

Files:
	764 created (1 per second)
		Creation alone: 500 files (2 per second)
		Mixed with transactions: 264 files (0 per second)
	243 read (0 per second)
	257 appended (0 per second)
	764 deleted (1 per second)
		Deletion alone: 528 files (4 per second)
		Mixed with transactions: 236 files (0 per second)

Data:
	1.36 megabytes read (2.07 kilobytes per second)
	4.45 megabytes written (6.73 kilobytes per second)

=====WLAN NFS==========================

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	375 seconds total
	172 seconds of transactions (2 per second)

Files:
	764 created (2 per second)
		Creation alone: 500 files (2 per second)
		Mixed with transactions: 264 files (1 per second)
	243 read (1 per second)
	257 appended (1 per second)
	764 deleted (2 per second)
		Deletion alone: 528 files (15 per second)
		Mixed with transactions: 236 files (1 per second)

Data:
	1.36 megabytes read (3.73 kilobytes per second)
	4.45 megabytes written (12.14 kilobytes per second)

=====LAN Kabi=========================

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	21 seconds total
	9 seconds of transactions (55 per second)

Files:
	764 created (36 per second)
		Creation alone: 500 files (55 per second)
		Mixed with transactions: 264 files (29 per second)
	243 read (27 per second)
	257 appended (28 per second)
	764 deleted (36 per second)
		Deletion alone: 528 files (176 per second)
		Mixed with transactions: 236 files (26 per second)

Data:
	1.36 megabytes read (66.54 kilobytes per second)
	4.45 megabytes written (216.80 kilobytes per second)


======LAN NFS========================

Creating files...Done
Performing transactions..........Done
Deleting files...Done
Time:
	21 seconds total
	10 seconds of transactions (50 per second)

Files:
	764 created (36 per second)
		Creation alone: 500 files (45 per second)
		Mixed with transactions: 264 files (26 per second)
	243 read (24 per second)
	257 appended (25 per second)
	764 deleted (36 per second)
		Deletion alone: 528 files (528 per second)
		Mixed with transactions: 236 files (23 per second)

Data:
	1.36 megabytes read (66.54 kilobytes per second)
	4.45 megabytes written (216.80 kilobytes per second)


\section {Snapshot Efficiency}

    The amount of space occupied by a snapshot is an important measurement of how efficient the snapshot system is. In this chapter, we will focus on the actual space occupied by the snapshot.
    
    Sections that are not referring to a whole block like the third section in \fref{fig:rsync} will be named ``tuncated section'' here after.

    To monitor the space occupied by a snapshot, we will do experiments using the following steps:

\begin{enumerate}
	\item Initialize the file system with block size $B$.

	\item Generate a file with size $F$.

	\item Fill the file with sections and let a certain proportion ($P$) of the sections be truncated.

	\item Take a snaphot of the file and make two branches.
	
	\item Switch to side branch 1, insert random number of bytes into the file at random offset. The offset value will be randomly picked in the file (uniform distribution). The number of bytes being inserted is also a uniforml distributed random variable between 1 to $F$.

	\item Take a snapshot on side branch 1 and calculate the total space occupied by this snapshot.

	\item Switch to side branch 2, overwrite random number of bytes from random offset. The offset value is also randomly picked in the file and the number of bytes being inserted is uniformly distributed between $1$ to largest possible value (i.e., until the end of the file).

	\item Take a snapshot on side branch 2 and calculate the total space occupied by this snapshot.

	\item Repeat above steps for 10,000 times to collect data (the average value).
\end{enumerate}

    \tref{tab:sample_result} shows two sample results of such experiment. The first three columns in the table are the variables of the experiment. The later four columns are the data gathered from the experiment. For example, the column labeled ``overwrite, classic'' means correspnding write operation is an overwrite and the algorithm used by snapshot system is classic copy-on-write.

    The first row in \tref{tab:sample_result} represents an experiment on a Kabi File System initialized with 128-byte block size. The target file is a 12,608-byte file where 3\% of all sections are truncated. The result shows that on average it takes a classic copy-on-write snapshot system 3,288 bytes to take a snapshot after an overwrite operation. It takes 103 bytes more for a Kabi File System to take a snapshot under the same condition. When it comes to insertion, on average it only takes the Kabi File System 3,256 bytes to take a snapshot after an insertion while it costs almost 2 times more space for a classic copy-on-write snapshot system to do so.

    This result is not difficult to explain. Because there is not much duplicated data in the overwrite scenario, SHA hashs and rolling checksums become overheads. This makes the performance of Kabi File System a little lower than the classic approach. But in the insertion scenario, lots of duplicated data can be found. The rsync algorithm is able to find the duplications to improve the efficiency. On the contrary, the classic approach can not do anything. So the Kabi File System have a better perfromance when it comes to insertion.
    
    The second row in \tref{tab:sample_result} represents another experiment different from the first one in file size. The file size in the second row equals to 126,080 bytes. It is obvious that when the file size becomes larger, the amount of space that is used to take a snapshot also become larger. However, this makes the comparasion between two experiments of different file size not so obvious.Therefore We introduce the normalized data as shown in \tref{tab:norm}. The major difference between \tref{tab:norm} and \tref{tab:sample_result} is that in \tref{tab:norm} we use the size of the file to normailze the result data so that we can compare results from different experiments easier.

\begin{lscape} 
\begin{table}
\caption{Sample result of the experiment}
\label{tab:sample_result}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12608 & 3\% & 3288 & 3391 & 9530 & 3256 \\
\hline
128 & 126080 & 3\% & 31722 & 32205 & 94867 & 32557 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{Normalized data}
\label{tab:norm}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12608 & 3\% & 0.2607 & 0.2689 & 0.7558 & 0.2582 \\
\hline
128 & 126080 & 3\% & 0.2516 & 0.2554 & 0.7524 & 0.2582 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\subsection{Block Size and File Size}

    The block size and the file size also influence the efficiency of a snapshot system. A high file-size-to-block-size ratio usually means fine-grained blocks. A classic copy-on-write snapshot will get better efficiency if it is a fine-grained target file. Consider the extreme case where the file-size-to-block-size ratio is 1. Then the file contains exactly one block. Therefore any change means a complete rewrite to the file by classic copy-on-write strategy. Column 4 and 6 in \tref{tab:fb_ratio} reflects this influence where high file-size-to-block-size retio improves the efficiency of classic copy-on-write system.

    However, column 7 shows that there is no obvious relationship between file-size-to-block-size ratio and efficiency when doing an insertion in Kabi File System. One reason for this could be the Kabi File System can tuncate block into smaller sections freely as shown in \fref{fig:buffer}. Therefore the Kabi File System can have fine-grained sections even though the file system uses a large block size.
    
    The data in column 5 reflects the fact that an increase in file-size-to-block-size ratio will result in an improvement in efficiency. The reason could be that the proportion of the metadata will reduces as the file size increases. For example, an overwrite operation can result in at most two additional sections added to the file node. This is an extra 56-byte metadata overhead. Compared to a 704-byte file, such overhead is large. On the other hand this overhead can be omittedcompared to a 14,008-byte file.

\begin{lscape} 
\begin{table}
\caption{File Size to Block Size ratio}
\label{tab:fb_ratio}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 704 & 17\% & 0.4531 & 0.5127 & 0.8338 & 0.2983 \\
\hline
128 & 1408 & 17\% & 0.3514 & 0.3934 & 0.7933 & 0.2962 \\
\hline
128 & 2112 & 17\% & 0.3129 & 0.3532 & 0.7789 & 0.2964 \\
\hline
128 & 3520 & 17\% & 0.2884 & 0.3213 & 0.7702 & 0.2972 \\
\hline
128 & 7040 & 17\% & 0.2681 & 0.2949 & 0.7589 & 0.2955 \\
\hline
128 & 14008 & 17\% & 0.2593 & 0.2828 & 0.7545 & 0.2957 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\subsection{Truncate Ratio}

    The rsync algorithm searches duplication in local buffer and remote sections. It uses the rolling checksum of sections to find duplications. But a truncated section does not have a valid rolling checksum thus it cannot be benifited from the rsync algorithm. Hence We can infer that the more truncated sections we have, the less efficient the snapshot system will be.

    \tref{tab:truncate_ratio} supports this inference. It shows the relationship between the efficiency and tuncated sections. Because a classic copy-on-write snapshot system does not have ``truncated section'', they are not infulenced by this experiment variable. On the other hand, tuncated sections significantly influence the efficiency of Kabi File System. It makes rsync algorithm less powerful. When all sections are truncated, the rsync algorithm no longer provide any extra efficiency in insert operation.

    In this experiment, we explicilty set the size of tuncated section to be half of the block size. That means for a Kabi File System with a 128-byte block size, the overhead (28 bytes) in metadata is almost half of the data contains in truncated section. In other words, for every 2 bytes stored in tuncated section, there is 1 extra byte cost in metadata. Since the increase in truncated-to-untruncated ratio implies more truncated section which is less efficient, this explains the significent decrease in efficiency in column 5 of \tref{tab:truncate_ratio}.

\begin{lscape} 
\begin{table}
\caption{Truncate Ratio}
\label{tab:truncate_ratio}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{3}{|c|}{experiment variables} & \multicolumn{4}{c|}{write operation and algorithm used} \\
\hline
block size & file size & truncated section & overwrite, classic & overwrite, Kabi & insert, classic & insert, Kabi\\
\hline
128 & 12800 & 0\% & 0.2596 & 0.2645 & 0.7547 & 0.2495 \\
\hline
128 & 12800 & 10\% & 0.2600 & 0.2752 & 0.7558 & 0.2750 \\
\hline
128 & 12800 & 18\% & 0.2599 & 0.2856 & 0.7557 & 0.3002 \\
\hline
128 & 12800 & 33\% & 0.2606 & 0.3071 & 0.7569 & 0.3513 \\
\hline
128 & 12800 & 46\% & 0.2616 & 0.3290 & 0.7579 & 0.4026 \\
\hline
128 & 12800 & 57\% & 0.2616 & 0.3500 & 0.7583 & 0.4532 \\
\hline
128 & 12800 & 71\% & 0.2596 & 0.3792 & 0.7530 & 0.5252 \\
\hline
128 & 12800 & 82\% & 0.2579 & 0.4089 & 0.7505 & 0.5980 \\
\hline
128 & 12800 & 91\% & 0.2593 & 0.4417 & 0.7536 & 0.6758 \\
\hline
128 & 12800 & 100\% & 0.2601 & 0.4737 & 0.7561 & 0.7536 \\
\hline
\end{tabular}
\end{center}
\end{table}
\end{lscape}

\section{Conclusion}

    In this Chapter, we showed the effects of three importent factors that affect the snapshot efficiency. The three factors are the block size, the file-size-to-block-size ratio and the proportion of truncated sections.

    Generally, in Kabi snapshot system an increase in block size or an increase in file-size-to-block-size ratio will make the overheads of metadata more efficient. The proportion of truncated sections is the most important factor among the three factors. An increase in this proportion will significently decrease the efficiency of the snapshot system in Kabi File System.
